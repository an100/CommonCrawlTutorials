{
 "metadata": {
  "name": "",
  "signature": "sha256:9b161742fd97661fa60dd14eee554432de824cd84d41a539b89e6b5834d762f2"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from __future__ import print_function"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import boto\n",
      "from boto.s3.connection import S3Connection\n",
      "\n",
      "from itertools import islice\n",
      "\n",
      "conn = S3Connection(anon=True)\n",
      "bucket = conn.get_bucket('aws-publicdatasets')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# a function to help with looking through S3 buckets\n",
      "\n",
      "from itertools import islice\n",
      "\n",
      "def keys_in_bucket(bucket, prefix, truncate_path=True, limit=None):\n",
      "    \"\"\"\n",
      "    given a S3 boto bucket, yields the keys in bucket with the prefix\n",
      "    if truncate_path is True, remove prefix in keys\n",
      "    optional limit to number of keys to return\n",
      "    \"\"\"\n",
      "    keys = islice(bucket.list(prefix=prefix, \n",
      "                                   delimiter=\"/\"),\n",
      "                       limit)\n",
      "    for key in keys:\n",
      "        name = key.name.encode(\"UTF-8\") \n",
      "        if truncate_path:\n",
      "            name = name.replace(prefix, \"\", 1)\n",
      "        yield name\n",
      "            "
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "What are the complete range of data for CC?"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The top level bucket is `s3://aws-publicdatasets/common-crawl/`"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "!s3cmd ls s3://aws-publicdatasets/common-crawl/"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# using boto to pull up the same list of buckets\n",
      "\n",
      "list(keys_in_bucket(bucket, \"common-crawl/\"))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Three eras of CC crawls \n",
      "\n",
      "within s3://aws-publicdatasets/common-crawl/\n",
      "\n",
      "I think of 3 phases:\n",
      "\n",
      "* pre-2012\n",
      "* 2012 corpus\n",
      "* Nutch era (after [transition to nutch](http://commoncrawl.org/common-crawl-move-to-nutch/) -- http://commoncrawl.org/new-crawl-data-available/)\n",
      "\n",
      "Let's look this up\n",
      "\n",
      "* ??? : crawl-001\n",
      "* ??? : crawl-002\n",
      "* 2012: parse-output  http://commoncrawl.org/2012-crawl-data-now-available/"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# pre - 2012\n",
      "\n",
      "!s3cmd ls s3://aws-publicdatasets/common-crawl/crawl-001/"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# pre - 2012\n",
      "\n",
      "!s3cmd ls s3://aws-publicdatasets/common-crawl/crawl-002/"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# 2012: https://commoncrawl.atlassian.net/wiki/display/CRWL/About+the+Data+Set\n",
      "!s3cmd ls s3://aws-publicdatasets/common-crawl/parse-output/"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "!s3cmd ls s3://aws-publicdatasets/common-crawl/crawl-data/ | grep CC-MAIN"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "2012 Crawl"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# for the 2012 data that I was working with in 2013.\n",
      "# not a good idea to take the contents for \"common-crawl/parse-output/segment/\" as valid segments\n",
      "# instead -- depend on s3://aws-publicdatasets/common-crawl/parse-output/valid_segments.txt\n",
      "\n",
      "# comment out so I won't run \n",
      "\n",
      "# for (i,key) in enumerate(islice(keys_in_bucket(bucket, \"common-crawl/parse-output/segment/\"),\n",
      "#                                 10)):\n",
      "#     print (i, key)\n",
      "    \n",
      "#!s3cmd ls s3://aws-publicdatasets/common-crawl/parse-output/"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "!s3cmd ls s3://aws-publicdatasets/common-crawl/parse-output/valid_segments.txt"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "k = bucket.get_key(\"common-crawl/parse-output/valid_segments.txt\")\n",
      "s = k.get_contents_as_string()\n",
      "\n",
      "valid_segments = filter(None, s.split(\"\\n\"))\n",
      "\n",
      "print (len(valid_segments), valid_segments[0])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# April 2014 crawl\n",
      "\n",
      "\n",
      "http://commoncrawl.org/april-2014-crawl-data-available/\n",
      "\n",
      "*    all segments (CC-MAIN-2014-15/segment.paths.gz)\n",
      "*    all WARC files (CC-MAIN-2014-15/warc.paths.gz)\n",
      "*    all WAT files (CC-MAIN-2014-15/wat.paths.gz)\n",
      "*    all WET files (CC-MAIN-2014-15/wet.paths.gz)\n",
      "\n",
      "https://aws-publicdatasets.s3.amazonaws.com/common-crawl/crawl-data/CC-MAIN-2014-15/segment.paths.gz"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "list(keys_in_bucket(bucket, \"common-crawl/crawl-data/CC-MAIN-2014-15/\"))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Do all of the nutch era crawls follow the same structure?"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "[key for key in keys_in_bucket(bucket, \"common-crawl/crawl-data/\") if key.startswith(\"CC-MAIN\")]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# convert year/week number to datetime\n",
      "\n",
      "import datetime\n",
      "\n",
      "def week_in_year(year, week):\n",
      "    return datetime.datetime(year,1,1) + datetime.timedelta((week-1)*7)\n",
      "\n",
      "week_in_year(2014,15)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import sys\n",
      "sys.maxint"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import gzip\n",
      "import StringIO\n",
      "\n",
      "# the following functions \n",
      "\n",
      "def gzip_from_key(bucket, key_name):\n",
      "    k = bucket.get_key(key_name)\n",
      "    f = gzip.GzipFile(fileobj=StringIO.StringIO(k.get_contents_as_string()))\n",
      "    return f\n",
      "\n",
      "def segments_from_gzip(gz):\n",
      "    s = gz.read()\n",
      "    return filter(None, s.split(\"\\n\"))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# let's parse segment.paths.gz\n",
      "\n",
      "valid_segments = segments_from_gzip(gzip_from_key(bucket, \n",
      "                    \"common-crawl/crawl-data/CC-MAIN-2014-15/segment.paths.gz\"))\n",
      "valid_segments[:5]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# rewrite to deal with gzip on the fly?\n",
      "# DON'T KNOW WHY THIS DOESN'T WORK -- COME BACK\n",
      "\n",
      "from boto.s3.key import Key\n",
      "from gzipstream import GzipStreamFile\n",
      "\n",
      "def gzip_from_key_2(bucket_name, key_name):\n",
      "\n",
      "    pds = conn.get_bucket(bucket_name)\n",
      "    k = Key(pds)\n",
      "    k.key = key_name    \n",
      "\n",
      "    f = GzipStreamFile(k)\n",
      "    \n",
      "    return f\n",
      "\n",
      "def segments_from_gzip_2(gz):\n",
      "    BUF_SIZE = 1000\n",
      "    s = \"\"\n",
      "    buffer = gz.read(BUF_SIZE)\n",
      "    while buffer:\n",
      "        s += buffer\n",
      "        buffer = gz.read(BUF_SIZE)\n",
      "\n",
      "    return filter(None, s.split(\"\\n\"))\n",
      "\n",
      "valid_segments_2 = segments_from_gzip_2(gzip_from_key_2(bucket, \n",
      "                    \"common-crawl/crawl-data/CC-MAIN-2014-15/segment.paths.gz\"))\n",
      "valid_segments_2[:5]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "WAT:  metadata files"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "all WAT files (CC-MAIN-2014-15/wat.paths.gz)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "wat_segments = segments_from_gzip(gzip_from_key(bucket, \n",
      "                    \"common-crawl/crawl-data/CC-MAIN-2014-15/wat.paths.gz\"))\n",
      "len(wat_segments)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "wat_segments[0]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "http://commoncrawl.org/navigating-the-warc-file-format/\n",
      "\n",
      "json file\n",
      "\n",
      "forgot to check how big the file is before downloading it.\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "k = bucket.get_key(wat_segments[0])\n",
      "k.size"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def get_key_sizes(bucket, keys):\n",
      "    sizes = []\n",
      "    for key in keys:\n",
      "        k = bucket.get_key(key)\n",
      "        sizes.append((key, k.size if hasattr(k, 'size') else 0))\n",
      "    return sizes"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "get_key_sizes(bucket, wat_segments[0:20])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# http://stackoverflow.com/questions/2348317/how-to-write-a-pager-for-python-iterators/2350904#2350904        \n",
      "def grouper(iterable, page_size):\n",
      "    page= []\n",
      "    for item in iterable:\n",
      "        page.append( item )\n",
      "        if len(page) == page_size:\n",
      "            yield page\n",
      "            page= []\n",
      "    if len(page) > 0:\n",
      "        yield page\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "multiprocessing approach"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "when to use Processes vs Threads?  I think you use threads when there's no danger of memory collision\n",
      "\n",
      "http://sebastianraschka.com/Articles/2014_multiprocessing_intro.html#Multi-Threading-vs.-Multi-Processing:\n",
      "\n",
      ">  If we submit \"jobs\" to different threads, those jobs can be pictured as \"sub-tasks\" of a single process and those threads will usually have access to the same memory areas (i.e., shared memory). This approach can easily lead to conflicts in case of improper synchronization, for example, if processes are writing to the same memory location at the same time."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "wat_segments[0:2]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "multiprocessing async approach"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# with pool.map\n",
      "\n",
      "from multiprocessing.dummy import Pool as ThreadPool \n",
      "from functools import partial\n",
      "\n",
      "get_key_sizes_for_bucket = partial(get_key_sizes, bucket)\n",
      "\n",
      "PAGE_SIZE = 1\n",
      "POOL_SIZE = 100\n",
      "MAX_SEGMENTS = 100\n",
      "\n",
      "pool = ThreadPool(POOL_SIZE) \n",
      "results = pool.map(get_key_sizes_for_bucket, grouper(wat_segments[0:MAX_SEGMENTS], PAGE_SIZE))\n",
      "pool.close()\n",
      "pool.join()\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Pool.imap_unordered\n",
      "\n",
      "from multiprocessing.dummy import Pool as ThreadPool \n",
      "from functools import partial\n",
      "\n",
      "get_key_sizes_for_bucket = partial(get_key_sizes, bucket)\n",
      "\n",
      "PAGE_SIZE = 1\n",
      "POOL_SIZE = 100\n",
      "MAX_SEGMENTS = None\n",
      "CHUNK_SIZE = 10\n",
      "\n",
      "pool = ThreadPool(POOL_SIZE) \n",
      "results_iter = pool.imap_unordered(get_key_sizes_for_bucket, \n",
      "                              grouper(wat_segments[0:MAX_SEGMENTS], PAGE_SIZE),\n",
      "                              CHUNK_SIZE)\n",
      "\n",
      "results = []\n",
      "                             \n",
      "for (i, page) in enumerate(islice(results_iter,None)):\n",
      "    print (i),\n",
      "    for result in page:\n",
      "        results.append(result)\n",
      "            \n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "results"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from pandas import Series, DataFrame\n",
      "df = DataFrame(results, columns=['key', 'size'])\n",
      "df.head()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df['size'].describe()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%pylab --no-import-all inline"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pylab as P\n",
      "P.hist(df['size'])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# total byte size of all the wat files\n",
      "print (format(sum(df['size']),\",d\"))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# save results\n",
      "import csv\n",
      "with open('CC-MAIN-2014-15.wat.csv', 'wb') as csvfile:\n",
      "    wat_writer = csv.writer(csvfile, delimiter=',',\n",
      "                            quotechar='|', quoting=csv.QUOTE_MINIMAL)\n",
      "    wat_writer.writerow(['key', 'size'])\n",
      "    for result in results:\n",
      "        wat_writer.writerow(result)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "!head CC-MAIN-2014-15.wat.csv"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Taking apart a WAT file"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "key = bucket.get_key(wat_segments[0])\n",
      "url = key.generate_url(expires_in=0, query_auth=False)\n",
      "url"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "wat_segments[0]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# looks like within any segment ID, we should expect warc, wat, wet buckets\n",
      "\n",
      "!s3cmd ls s3://aws-publicdatasets/common-crawl/crawl-data/CC-MAIN-2014-15/segments/1397609521512.15/"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from gzipstream import GzipStreamFile\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import boto\n",
      "from itertools import islice\n",
      "from boto.s3.key import Key\n",
      "from gzipstream import GzipStreamFile\n",
      "import warc\n",
      "import json\n",
      "\n",
      "\n",
      "def test_gzipstream():\n",
      "\n",
      "  output = []\n",
      "    \n",
      "  # Let's use a random gzipped web archive (WARC) file from the 2014-15 Common Crawl dataset\n",
      "  ## Connect to Amazon S3 using anonymous credentials\n",
      "  conn = boto.connect_s3(anon=True)\n",
      "  pds = conn.get_bucket('aws-publicdatasets')\n",
      "  ## Start a connection to one of the WARC files\n",
      "  k = Key(pds)\n",
      "  k.key = 'common-crawl/crawl-data/CC-MAIN-2014-15/segments/1397609521512.15/warc/CC-MAIN-20140416005201-00000-ip-10-147-4-33.ec2.internal.warc.gz'\n",
      "\n",
      "  # The warc library accepts file like objects, so let's use GzipStreamFile\n",
      "  f = warc.WARCFile(fileobj=GzipStreamFile(k))\n",
      "  for num, record in islice(enumerate(f),100):\n",
      "    if record['WARC-Type'] == 'response':\n",
      "      # Imagine we're interested in the URL, the length of content, and any Content-Type strings in there\n",
      "      output.append((record['WARC-Target-URI'], record['Content-Length']))\n",
      "      output.append( '\\n'.join(x for x in record.payload.read().replace('\\r', '').split('\\n\\n')[0].split('\\n') if 'content-type:' in x.lower()))\n",
      "      output.append( '=-=-' * 10)\n",
      " \n",
      "  return output\n",
      "        "
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def warc_records(key_name, limit=None):\n",
      "    conn = boto.connect_s3(anon=True)\n",
      "    bucket = conn.get_bucket('aws-publicdatasets')\n",
      "    key = bucket.get_key(key_name)\n",
      "\n",
      "    # The warc library accepts file like objects, so let's use GzipStreamFile\n",
      "    f = warc.WARCFile(fileobj=GzipStreamFile(key))\n",
      "    for record in islice(f, limit):\n",
      "        yield record"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# let's compute some stats on the headers\n",
      "from collections import Counter\n",
      "c=Counter()\n",
      "[c.update(record.header.keys()) for record in warc_records(wat_segments[0],100)]\n",
      "c"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# warc-type\n",
      "\n",
      "# looks like \n",
      "# first record is 'warcinfo\n",
      "# rest is 'metadata'\n",
      "\n",
      "c=Counter()\n",
      "[c.update([record['warc-type']]) for record in warc_records(wat_segments[0],100)]\n",
      "\n",
      "c"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# content-type\n",
      "\n",
      "c=Counter()\n",
      "[c.update([record['content-type']]) for record in warc_records(wat_segments[0],100)]\n",
      "\n",
      "c"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# what"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "wrecords = warc_records(wat_segments[0])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "record = wrecords.next()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# http://warc.readthedocs.org/en/latest/#working-with-warc-header\n",
      "\n",
      "record.header.items()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#payload\n",
      "dir(record.payload)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "record.header.get('content-type')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# payload\n",
      "\n",
      "\n",
      "s = record.payload.read()\n",
      "len(s)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "if record.header.get('content-type') == 'application/json':\n",
      "    payload = json.loads(s)\n",
      "else:\n",
      "    payload = s\n",
      "    \n",
      "payload"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "payload['Envelope']['WARC-Header-Metadata']['WARC-Target-URI']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import urlparse\n",
      "urlparse.urlparse(payload['Envelope']['WARC-Header-Metadata']['WARC-Target-URI']).netloc"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import urlparse\n",
      "\n",
      "def netloc_count_for_segment(segment_id, limit=None):\n",
      "    \n",
      "    c = Counter()\n",
      "\n",
      "    for record in warc_records(segment_id,limit):\n",
      "\n",
      "        s = record.payload.read()\n",
      "        if record.header.get('content-type') == 'application/json':\n",
      "            payload = json.loads(s)\n",
      "            url = payload['Envelope']['WARC-Header-Metadata'].get('WARC-Target-URI')\n",
      "            if url:\n",
      "                netloc = urlparse.urlparse(url).netloc\n",
      "                c.update([netloc])\n",
      "            else:\n",
      "                c.update([None])\n",
      "\n",
      "    return c\n",
      "\n",
      "\n",
      "        "
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%time netloc_count_for_segment(wat_segments[0],100)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "apply(netloc_count_for_segment, [wat_segments[0],100])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def add(x, y):\n",
      "    return x  + y\n",
      "\n",
      "import multyvac\n",
      "jid = multyvac.submit(add, 2, 3 ,_core='c1', _name=\"just add: 2, 3\")\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "job = multyvac.get(jid)\n",
      "job.status"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "job.get_result()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import multyvac\n",
      "jid = multyvac.submit(netloc_count_for_segment, wat_segments[0],1500,_core='c1', _name=\"wat_seg:0, 1500, c1; new gzipstream\")\n",
      "jid"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "job = multyvac.get(jid)\n",
      "job"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "job.status"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "job.get_result()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "WARC files:  the final frontier"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Is it possible to work with pieces of the new warc files without having to download the whole file?  \n",
      "\n",
      "In the 2012 crawl data, there was an index built http://urlsearch.commoncrawl.org/ --> from which you can get a reference to an offset and length inside of the arc.gz file.  With S3, you don't need to then download the whole file but just a chunk....and that chunk itself is unpackable as a gzip file.  (nice feature of gzip?):\n",
      "\n",
      "http://nbviewer.ipython.org/github/rdhyee/working-open-data/blob/postscript/notebooks/Day_21_CommonCrawl_Content.ipynb#Grabbing-pieces-of-the-.arc.gz-files\n",
      "\n",
      "Is there a similar way of working with the 2014 crawl data?  That is, a way of reading off chunks of the warc file without grabbing entire warc files?"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": []
    }
   ],
   "metadata": {}
  }
 ]
}